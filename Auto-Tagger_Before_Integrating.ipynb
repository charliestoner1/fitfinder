{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac63ffc9-535e-479d-90b5-798235676809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling transformers & tokenizers (if present)...\n",
      "Installing transformers==4.49.0 and tokenizers==0.21.4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.49.0 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: tokenizers==0.21.4 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (0.21.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from transformers==4.49.0) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from transformers==4.49.0) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from transformers==4.49.0) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from transformers==4.49.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from transformers==4.49.0) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from transformers==4.49.0) (2025.10.23)\n",
      "Requirement already satisfied: requests in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from transformers==4.49.0) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from transformers==4.49.0) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from transformers==4.49.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from tqdm>=4.27->transformers==4.49.0) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from requests->transformers==4.49.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from requests->transformers==4.49.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from requests->transformers==4.49.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from requests->transformers==4.49.0) (2025.10.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.49.0\n",
      "tokenizers:   0.21.4\n",
      "Installing einops + timm...\n",
      "Requirement already satisfied: einops in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: timm in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (1.0.20)\n",
      "Requirement already satisfied: torch in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from timm) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from timm) (0.20.1+cu121)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from timm) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from timm) (0.35.3)\n",
      "Requirement already satisfied: safetensors in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from timm) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from huggingface_hub->timm) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from huggingface_hub->timm) (2025.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from huggingface_hub->timm) (25.0)\n",
      "Requirement already satisfied: requests in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from huggingface_hub->timm) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from huggingface_hub->timm) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from requests->huggingface_hub->timm) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from requests->huggingface_hub->timm) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from requests->huggingface_hub->timm) (2025.10.5)\n",
      "Requirement already satisfied: networkx in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from torchvision->timm) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\fashi\\anaconda3\\envs\\fitfinder\\lib\\site-packages (from torchvision->timm) (11.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Section 0 - Reset + Pin exact versions\n",
    "import sys, os, importlib, shutil, pathlib\n",
    "\n",
    "# Use this interpreter's pip \n",
    "pip = f'\"{sys.executable}\" -m pip'\n",
    "\n",
    "# 1) Cleanly remove any preloaded modules from memory\n",
    "for m in list(sys.modules):\n",
    "    if m.startswith(\"transformers\") or m.startswith(\"tokenizers\"):\n",
    "        del sys.modules[m]\n",
    "\n",
    "# 2) Uninstall then install the pinned versions\n",
    "print(\"Uninstalling transformers & tokenizers (if present)...\")\n",
    "!{pip} uninstall -y transformers tokenizers >/dev/null\n",
    "\n",
    "print(\"Installing transformers==4.49.0 and tokenizers==0.21.4 ...\")\n",
    "!{pip} install --no-cache-dir \"transformers==4.49.0\" \"tokenizers==0.21.4\"\n",
    "\n",
    "# 3) Verify versions\n",
    "import transformers, tokenizers\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"tokenizers:  \", tokenizers.__version__)\n",
    "\n",
    "print(\"Installing einops + timm...\")\n",
    "!{pip} install --no-cache-dir einops timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda5bc0c-798d-4901-b949-576d3e70b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1 - Variables\n",
    "\n",
    "# --- Model / Task ---\n",
    "MODEL_ID: str           = \"microsoft/Florence-2-base\"\n",
    "TASK_CAPTION: str       = \"<MORE_DETAILED_CAPTION>\"                 # or just <CAPTION>\n",
    "IMAGE_PLACEHOLDER: str  = \"<image>\"                                 # REQUIRED token for Florence-2 prompts\n",
    "REVISION                = \"main\"\n",
    "\n",
    "# --- Inference knobs ---\n",
    "MAX_NEW_TOKENS: int     = 128\n",
    "NUM_BEAMS: int          = 3\n",
    "RESIZE_LONG_SIDE: int   = 512\n",
    "\n",
    "# --- Device / dtype preferences ---\n",
    "PREFERRED_DTYPE: str    = \"float32\"                             \n",
    "FORCE_CPU: bool         = False\n",
    "\n",
    "# --- Image sources ---\n",
    "TRAIN_DIR: str          = r\"F:\\deepfashion2\\images\\train\\train\\image\"\n",
    "N_SAMPLES: int          = 15\n",
    "\n",
    "# --- Output / logging ---\n",
    "SAVE_JSON_PATH: str     = \"florence_preview_output.json\"\n",
    "PRINT_DEBUG: bool       = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7638bf2f-74cb-4a82-b0b4-73970d6792c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Runtime / Versions ===\n",
      "Python:         3.10.18  (Windows 10)\n",
      "PyTorch:        2.5.1+cu121\n",
      "Transformers:   4.49.0\n",
      "Pillow (PIL):   11.3.0\n",
      "Matplotlib:     3.10.7\n",
      "TorchVision:    0.20.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# Section 2 - Imports & Version Checks\n",
    "\n",
    "import os, sys, re, json, platform, importlib, math, warnings\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ.setdefault(\"HF_HUB_DISABLE_SYMLINKS_WARNING\", \"1\")\n",
    "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub.file_download\")\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import transformers\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _get_ver(name: str) -> Optional[str]:\n",
    "    try:\n",
    "        return importlib.import_module(name).__version__\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "python_ver = sys.version.split()[0]\n",
    "print(\"=== Runtime / Versions ===\")\n",
    "print(f\"Python:         {python_ver}  ({platform.system()} {platform.release()})\")\n",
    "print(f\"PyTorch:        {_get_ver('torch')}\")\n",
    "print(f\"Transformers:   {_get_ver('transformers')}\")\n",
    "print(f\"Pillow (PIL):   {_get_ver('PIL')}\")\n",
    "print(f\"Matplotlib:     {_get_ver('matplotlib')}\")\n",
    "print(f\"TorchVision:    {_get_ver('torchvision')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f0f7bfb-d94a-42d5-ba9b-74e023b53547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hardware ===\n",
      "Selected device: cuda:0\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Section 3 - Device, DType, and Image Utilities\n",
    "\n",
    "if FORCE_CPU:\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else (\n",
    "        torch.device(\"mps\") if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "    )\n",
    "\n",
    "# --- DType selection ---\n",
    "def pick_dtype(pref: str, device: torch.device) -> torch.dtype:\n",
    "    pref = pref.lower()\n",
    "    if device.type == \"cuda\":\n",
    "        # bf16 support is only on newer GPUs\n",
    "        bf16_ok = hasattr(torch.cuda, \"is_bf16_supported\") and torch.cuda.is_bf16_supported()\n",
    "        if pref == \"bfloat16\" and bf16_ok:\n",
    "            return torch.bfloat16\n",
    "        if pref in (\"float16\", \"bfloat16\") and not bf16_ok:\n",
    "            return torch.float16  # good default on CUDA if bf16 not supported\n",
    "        return torch.float16 if pref != \"float32\" else torch.float32\n",
    "    elif device.type == \"mps\":\n",
    "        # MPS generally prefers float32 for safety\n",
    "        return torch.float32\n",
    "    else:\n",
    "        # CPU fallback\n",
    "        return torch.float32\n",
    "\n",
    "dtype = pick_dtype(PREFERRED_DTYPE, device)\n",
    "\n",
    "print(\"=== Hardware ===\")\n",
    "print(f\"Selected device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"dtype: {dtype}\")\n",
    "\n",
    "# --- Image utilities ---\n",
    "def load_image(path: str) -> Image.Image:\n",
    "    \"\"\"Load an image from disk as RGB.\"\"\"\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "def resize_long_side(img: Image.Image, long_side: int = 512) -> Image.Image:\n",
    "    \"\"\"Resize the longer side of the image to `long_side` while maintaining aspect ratio.\"\"\"\n",
    "    w, h = img.size\n",
    "    if max(w, h) <= long_side:\n",
    "        return img\n",
    "    if w >= h:\n",
    "        new_w = long_side\n",
    "        new_h = int(h * (long_side / w))\n",
    "    else:\n",
    "        new_h = long_side\n",
    "        new_w = int(w * (long_side / h))\n",
    "    return img.resize((new_w, new_h), Image.BICUBIC)\n",
    "\n",
    "def discover_images(root: str, n: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively find up to n image files under `root`.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    roots_to_try = [root]\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "    for r in roots_to_try:\n",
    "        p = Path(r)\n",
    "        if not p.exists():\n",
    "            continue\n",
    "        for fp in p.rglob(\"*\"):\n",
    "            if fp.suffix.lower() in exts and (\"__MACOSX\" not in str(fp)):\n",
    "                candidates.append(str(fp))\n",
    "        if len(candidates) >= n:\n",
    "            break\n",
    "    candidates = sorted(set(candidates))\n",
    "    if len(candidates) < n:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not find {n} images under {root}. Found {len(candidates)}.\"\n",
    "        )\n",
    "    return candidates[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edfe9a9a-8a88-4a6a-b22f-9b4f9394ea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Florence-2 ===\n",
      "Model:      microsoft/Florence-2-base\n",
      "On device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Section 4 - Load Florence-2 (Processor + Model)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    revision=REVISION,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "load_dtype = dtype if (hasattr(torch, \"cuda\") and device.type != \"cpu\") else torch.float32\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    revision=REVISION,\n",
    "    torch_dtype=load_dtype,                 \n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"=== Florence-2 ===\")\n",
    "print(f\"Model:      {MODEL_ID}\")\n",
    "print(f\"On device:  {next(model.parameters()).device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f267fbe-c39a-48f3-a8cc-af4cd719ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5 - Garment-Focused Ontology + Normalizers + Caption -> Tags (types/colors/patterns)\n",
    "\n",
    "import re\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# --- Ontology (canonical forms on the right) ---\n",
    "TYPE_ALIASES = {\n",
    "    # tops\n",
    "    \"tee\": \"t-shirt\", \"tshirt\": \"t-shirt\", \"t-shirt\": \"t-shirt\", \"t shirt\": \"t-shirt\",\n",
    "    \"tee shirt\": \"t-shirt\", \"t- shirt\": \"t-shirt\", \"top\": \"top\",\n",
    "    \"blouse\": \"blouse\", \"shirt\": \"shirt\", \"polo\": \"polo shirt\",\n",
    "    \"hoodie\": \"hoodie\", \"sweatshirt\": \"sweatshirt\", \"sweater\": \"sweater\", \"cardigan\": \"cardigan\",\n",
    "    \"tank\": \"tank top\", \"tanktop\": \"tank top\", \"camisole\": \"camisole\", \"crop\": \"crop top\",\n",
    "    # outerwear\n",
    "    \"jacket\": \"jacket\", \"coat\": \"coat\", \"blazer\": \"blazer\", \"parka\": \"parka\",\n",
    "    \"windbreaker\": \"windbreaker\", \"raincoat\": \"raincoat\",\n",
    "    # dresses/one-piece\n",
    "    \"dress\": \"dress\", \"jumpsuit\": \"jumpsuit\", \"romper\": \"romper\",\n",
    "    # bottoms\n",
    "    \"jeans\": \"jeans\", \"pants\": \"pants\", \"trousers\": \"trousers\", \"slacks\": \"trousers\",\n",
    "    \"shorts\": \"shorts\", \"chinos\": \"pants\", \"leggings\": \"leggings\", \"skirt\": \"skirt\",\n",
    "    \"sweatpants\": \"sweatpants\", \"joggers\": \"joggers\",\n",
    "    # footwear\n",
    "    \"sneakers\": \"sneakers\", \"tennis\": \"sneakers\", \"running\": \"sneakers\", \"shoes\": \"shoes\",\n",
    "    \"boots\": \"boots\", \"ankle\": \"boots\", \"heels\": \"heels\", \"loafers\": \"loafers\", \"sandals\": \"sandals\",\n",
    "    # accessories\n",
    "    \"hat\": \"hat\", \"cap\": \"cap\", \"beanie\": \"beanie\", \"scarf\": \"scarf\",\n",
    "    \"gloves\": \"gloves\", \"belt\": \"belt\", \"bag\": \"bag\", \"backpack\": \"backpack\",\n",
    "    # parts/trim \n",
    "    \"hood\": \"hood\", \"collar\": \"collar\", \"sleeve\": \"sleeve\"\n",
    "}\n",
    "FASHION_TYPES = set(TYPE_ALIASES.values())\n",
    "\n",
    "COLOR_ALIASES = {\n",
    "    \"navy\": \"blue\", \"teal\": \"blue\", \"aqua\": \"blue\", \"cyan\": \"blue\", \"azure\": \"blue\", \"indigo\": \"blue\",\n",
    "    \"maroon\": \"red\", \"burgundy\": \"red\", \"crimson\": \"red\",\n",
    "    \"ivory\": \"white\", \"cream\": \"white\", \"off-white\": \"white\", \"off white\": \"white\",\n",
    "    \"charcoal\": \"grey\", \"slate\": \"grey\", \"silver\": \"grey\", \"gray\": \"grey\",\n",
    "    \"tan\": \"brown\", \"beige\": \"brown\", \"khaki\": \"brown\", \"camel\": \"brown\",\n",
    "    \"gold\": \"yellow\", \"blonde\": \"yellow\", \"blond\": \"yellow\",\n",
    "    \"violet\": \"purple\", \"magenta\": \"pink\", \"fuchsia\": \"pink\", \"mustard\": \"yellow\"\n",
    "}\n",
    "BASIC_COLORS = {\n",
    "    \"black\", \"white\", \"grey\", \"blue\", \"red\", \"green\", \"yellow\",\n",
    "    \"purple\", \"pink\", \"brown\", \"orange\"\n",
    "}\n",
    "\n",
    "# Patterns normalized to canonical names\n",
    "PATTERN_ALIASES = {\n",
    "    \"polka\": \"polka dot\", \"polka-dot\": \"polka dot\", \"polka dot\": \"polka dot\",\n",
    "    \"plaid\": \"plaid\", \"tartan\": \"plaid\",\n",
    "    \"striped\": \"striped\", \"stripes\": \"striped\",\n",
    "    \"checks\": \"checkered\", \"checked\": \"checkered\", \"checkered\": \"checkered\",\n",
    "    \"floral\": \"floral\", \"flowers\": \"floral\",\n",
    "    \"graphic\": \"graphic\", \"logo\": \"logo\", \"print\": \"graphic\",\n",
    "    \"camouflage\": \"camouflage\", \"camo\": \"camouflage\",\n",
    "    \"leopard\": \"animal print\", \"zebra\": \"animal print\", \"animal\": \"animal print\"\n",
    "}\n",
    "PATTERNS = set(PATTERN_ALIASES.values())\n",
    "\n",
    "# --- Heuristics for focusing on garment-related text ---\n",
    "CLOTHING_HINTS = set(TYPE_ALIASES.keys()) | FASHION_TYPES\n",
    "WEAR_VERBS = {\"wear\", \"wears\", \"wearing\", \"wore\", \"dressed\", \"dressing\", \"has on\"}\n",
    "\n",
    "NONCLOTHING_HINTS = {\n",
    "    \"bed\",\"blanket\",\"pillow\",\"wall\",\"floor\",\"sofa\",\"couch\",\"curtain\",\"bracelet\",\"earring\",\"necklace\",\n",
    "    \"hair\",\"skin\",\"makeup\",\"room\",\"kitchen\",\"bedroom\",\"living\",\"bathroom\",\"mirror\",\"background\",\n",
    "    \"hand\",\"hands\",\"arm\",\"arms\",\"wrist\",\"waist\",\"leg\",\"legs\",\"face\",\"faces\",\"eye\",\"eyes\"\n",
    "}\n",
    "\n",
    "# --- Normalizers ---\n",
    "def normalize_type(word: str) -> Optional[str]:\n",
    "    w = word.lower().strip().replace(\"-\", \" \")\n",
    "    return TYPE_ALIASES.get(w)\n",
    "\n",
    "def _has_wear_verb(s: str) -> bool:\n",
    "    return bool(re.search(r\"\\b(wear|wears|wearing|wore|dressed|dressing|has on)\\b\", s))\n",
    "\n",
    "def _adjacent_to(tokens, idx, vocab: set) -> bool:\n",
    "    \"\"\"True if token at idx has a left/right neighbor inside `vocab`.\"\"\"\n",
    "    left  = tokens[idx-1][0] if idx - 1 >= 0 else None\n",
    "    right = tokens[idx+1][0] if idx + 1 < len(tokens) else None\n",
    "    return (left in vocab) or (right in vocab)\n",
    "\n",
    "def normalize_color(word: str) -> Optional[str]:\n",
    "    w = word.lower().strip().replace(\"-\", \" \")\n",
    "    w = COLOR_ALIASES.get(w, w)\n",
    "    return w if w in BASIC_COLORS else None\n",
    "\n",
    "def normalize_pattern(word: str) -> Optional[str]:\n",
    "    w = word.lower().strip()\n",
    "    w = PATTERN_ALIASES.get(w, w)\n",
    "    return w if w in PATTERNS else None\n",
    "\n",
    "# --- Tokenization helpers ---\n",
    "def _tokens_with_spans(text: str) -> List[Tuple[str, int, int]]:\n",
    "    return [(m.group(0).lower(), m.start(), m.end())\n",
    "            for m in re.finditer(r\"[a-zA-Z]+(?:-[a-zA-Z]+)?\", text)]\n",
    "\n",
    "def _sentence_spans(text: str) -> List[Tuple[int, int]]:\n",
    "    spans, start = [], 0\n",
    "    for m in re.finditer(r\"(?<=[.!?])\\s+\", text):\n",
    "        end = m.start()\n",
    "        spans.append((start, end))\n",
    "        start = m.end()\n",
    "    spans.append((start, len(text)))\n",
    "    return spans\n",
    "\n",
    "# --- Main: garment-focused caption → tags ---\n",
    "def extract_tags_from_caption(caption: str,\n",
    "                              proximity_window: int = 6,\n",
    "                              max_colors: int = 4) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract types/colors/patterns from a caption, but only from:\n",
    "      1) sentences that contain clothing words or wear-verbs, and\n",
    "      2) tokens within a proximity window around clothing words.\n",
    "    Also ignores obvious background nouns (bed, bracelet, wall, etc.).\n",
    "    \"\"\"\n",
    "    text = caption.strip()\n",
    "    if not text:\n",
    "        return {\"type\": [], \"color\": [], \"pattern\": []}\n",
    "\n",
    "    tokens = _tokens_with_spans(text)            # [(tok, a, b), ...]\n",
    "    sents = _sentence_spans(text)                # [(a, b), ...]\n",
    "    text_l = text.lower()\n",
    "\n",
    "    # Mark sentences that talk about clothing\n",
    "    clothing_sent_idx = set()\n",
    "    for i, (a, b) in enumerate(sents):\n",
    "        s = text_l[a:b]\n",
    "        if _has_wear_verb(s) or any(w in s for w in CLOTHING_HINTS):\n",
    "            clothing_sent_idx.add(i)\n",
    "\n",
    "    # Collect token indices in relevant zones\n",
    "    relevant_idx = set()\n",
    "\n",
    "    # (A) proximity to clothing words\n",
    "    clothing_token_positions = [i for i, (tok, _, _) in enumerate(tokens) if tok in CLOTHING_HINTS]\n",
    "    for pos in clothing_token_positions:\n",
    "        lo = max(0, pos - proximity_window)\n",
    "        hi = min(len(tokens), pos + proximity_window + 1)\n",
    "        for j in range(lo, hi):\n",
    "            relevant_idx.add(j)\n",
    "\n",
    "    # (B) any token inside a clothing-marked sentence\n",
    "    for si in clothing_sent_idx:\n",
    "        sa, sb = sents[si]\n",
    "        for j, (_, a, b) in enumerate(tokens):\n",
    "            if a >= sa and b <= sb:\n",
    "                relevant_idx.add(j)\n",
    "\n",
    "    # Nothing relevant? bail early\n",
    "    if not relevant_idx:\n",
    "        return {\"type\": [], \"color\": [], \"pattern\": []}\n",
    "\n",
    "    types, colors, patterns = set(), set(), set()\n",
    "\n",
    "    # Scan only relevant tokens, skipping obvious background words\n",
    "    for idx, (tok, _, _) in enumerate(tokens):\n",
    "        if idx not in relevant_idx:\n",
    "            continue\n",
    "        if tok in NONCLOTHING_HINTS:\n",
    "            continue\n",
    "\n",
    "        # types\n",
    "        t = normalize_type(tok)\n",
    "        if t:\n",
    "            # canonicalize tee/t-shirt etc.\n",
    "            types.add(t)\n",
    "\n",
    "        # single-token patterns\n",
    "        p = normalize_pattern(tok)\n",
    "        if p:\n",
    "            if _adjacent_to(tokens, idx, NONCLOTHING_HINTS):   # e.g., \"plaid blanket\"\n",
    "                continue\n",
    "            patterns.add(p)\n",
    "\n",
    "        # colors\n",
    "        c = normalize_color(tok)\n",
    "        if c:\n",
    "            if _adjacent_to(tokens, idx, NONCLOTHING_HINTS):   # e.g., \"brown hair\", \"blue blanket\"\n",
    "                continue\n",
    "            colors.add(c)\n",
    "\n",
    "    # multi-word patterns (adjacency-aware)\n",
    "    rel_tokens = [tokens[i] for i in sorted(relevant_idx)]\n",
    "    rel_words  = [t[0] for t in rel_tokens]\n",
    "\n",
    "    def _phrase_present_not_adjacent_to_nonclothing(words: List[str], phrase: str) -> bool:\n",
    "        parts = phrase.split()\n",
    "        L = len(parts)\n",
    "        for i in range(len(words) - L + 1):\n",
    "            if words[i:i+L] == parts:\n",
    "                left  = words[i-1] if i-1 >= 0 else None\n",
    "                right = words[i+L] if i+L < len(words) else None\n",
    "                if (left in NONCLOTHING_HINTS) or (right in NONCLOTHING_HINTS):\n",
    "                    continue  # e.g., \"plaid [blanket]\"\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    for pat_phrase in [\"polka dot\",\"animal print\",\"checkered\",\"striped\",\"floral\",\"camouflage\",\"graphic\",\"logo\",\"plaid\"]:\n",
    "        if _phrase_present_not_adjacent_to_nonclothing(rel_words, pat_phrase):\n",
    "            patterns.add(normalize_pattern(pat_phrase))\n",
    "\n",
    "    # Heuristic: printed/text/letters near clothing ⇒ treat as logo/graphic\n",
    "    if re.search(r\"\\b(logo|graphic|print|printed|text|letters)\\b\", \" \".join(rel_words)):\n",
    "        patterns.add(\"logo\")\n",
    "    \n",
    "    # Post-process: collapse ambiguous types (e.g., \"top\" + \"t-shirt\" -> keep t-shirt)\n",
    "    if \"t-shirt\" in types and \"top\" in types:\n",
    "        types.discard(\"top\")\n",
    "\n",
    "    # Limit excessive colors (long captions can still mention many)\n",
    "    if len(colors) > max_colors:\n",
    "        colors = set(list(colors)[:max_colors])  # preserve arbitrary but bounded count\n",
    "\n",
    "    return {\n",
    "        \"type\":    sorted(types),\n",
    "        \"color\":   sorted(colors),\n",
    "        \"pattern\": sorted(p for p in patterns if p),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74b39257-4a6c-4380-a7df-57929a7ceddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6 - Florence-2 Inference Helpers (Caption → Tags) [FINAL PATCH]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def florence_generate_caption(\n",
    "    img: Image.Image,\n",
    "    task_token: str = TASK_CAPTION,         \n",
    "    max_new_tokens: int = MAX_NEW_TOKENS,\n",
    "    num_beams: int = NUM_BEAMS\n",
    ") -> str:\n",
    "    # Prompt must be the task token only; processor will expand it.\n",
    "    prompt = task_token\n",
    "\n",
    "    # Bound compute\n",
    "    img_resized = resize_long_side(img, RESIZE_LONG_SIDE)\n",
    "\n",
    "    # >>> IMPORTANT: pass images as a list (batch of 1) <<<\n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=[img_resized],               # <- list, not a single PIL image\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Align device + dtype with model (floats -> model dtype; ints -> moved only)\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "    aligned = {}\n",
    "    for k, v in inputs.items():\n",
    "        if torch.is_tensor(v):\n",
    "            aligned[k] = v.to(device=device, dtype=model_dtype) if torch.is_floating_point(v) else v.to(device)\n",
    "        else:\n",
    "            aligned[k] = v\n",
    "\n",
    "    # Generate caption\n",
    "    outputs = model.generate(\n",
    "        **aligned,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=num_beams,\n",
    "    )\n",
    "    caption = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    caption = caption.replace(\"<CAPTION>\", \"\").replace(\"<MORE_DETAILED_CAPTION>\", \"\").strip()\n",
    "    return caption\n",
    "\n",
    "def image_to_tags_and_caption(img: Image.Image):\n",
    "    caption = florence_generate_caption(img, TASK_CAPTION)\n",
    "    tags = extract_tags_from_caption(caption)\n",
    "    return tags, caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d3a727f-eb0b-401a-84bb-506f0a62bb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers_modules.microsoft.Florence-2-base.5ca5edf5bd017b9919c05d08aebef5e4c7ac3bac.processing_florence2.Florence2Processor'>\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(type(processor))                 # should show Florence2Processor\n",
    "print(next(model.parameters()).dtype)  # model dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b103a2c-60d2-46fc-ab9f-84d771ea4985",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Could not find 15 images under F:\\deepfashion2\\images\\train\\train\\image. Found 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Section 7 - Pick 3 Training Images, Run, and Display\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m sample_paths \u001b[38;5;241m=\u001b[39m \u001b[43mdiscover_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_SAMPLES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m PRINT_DEBUG:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Sample Images ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 73\u001b[0m, in \u001b[0;36mdiscover_images\u001b[1;34m(root, n)\u001b[0m\n\u001b[0;32m     71\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(candidates))\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(candidates) \u001b[38;5;241m<\u001b[39m n:\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images under \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(candidates)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m     )\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m candidates[:n]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Could not find 15 images under F:\\deepfashion2\\images\\train\\train\\image. Found 0."
     ]
    }
   ],
   "source": [
    "# Section 7 - Pick Training Images, Run, and Display\n",
    "\n",
    "sample_paths = discover_images(TRAIN_DIR, n=N_SAMPLES)\n",
    "if PRINT_DEBUG:\n",
    "    print(\"=== Sample Images ===\")\n",
    "    for i, p in enumerate(sample_paths, 1):\n",
    "        print(f\"{i}. {p}\")\n",
    "\n",
    "results: List[Dict[str, Any]] = []\n",
    "\n",
    "for idx, p in enumerate(sample_paths, 1):\n",
    "    img = load_image(p)\n",
    "    tags, caption = image_to_tags_and_caption(img)\n",
    "\n",
    "    # record\n",
    "    rec = {\"index\": idx, \"path\": p, \"caption\": caption, \"tags\": tags}\n",
    "    results.append(rec)\n",
    "\n",
    "    # show\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Image {idx}\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n--- Image {idx} ---\")\n",
    "    print(f\"Path:    {p}\")\n",
    "    print(f\"Caption: {caption}\")\n",
    "    print(f\"Tags:    {json.dumps(tags, ensure_ascii=False)}\")\n",
    "\n",
    "# optionally save\n",
    "with open(SAVE_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "if PRINT_DEBUG:\n",
    "    print(f\"\\nSaved results to: {SAVE_JSON_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f9b33-dea1-406d-abd0-4d3bb4397e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e158d-c2ca-4734-9b7d-f48b59b0ca15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fitfinder)",
   "language": "python",
   "name": "fitfinder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
